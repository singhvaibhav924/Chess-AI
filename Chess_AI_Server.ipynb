{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3m8kVQWUMyD"
      },
      "source": [
        "#Importing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ih9nV0BUZOyo"
      },
      "outputs": [],
      "source": [
        "!pip install flask-ngrok\n",
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtMpvhrJZvtk"
      },
      "outputs": [],
      "source": [
        "from flask_ngrok import run_with_ngrok\n",
        "from flask import Flask, request, send_file\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from keras import Model, optimizers\n",
        "from keras.models import load_model\n",
        "from keras.layers import *\n",
        "from keras.utils import plot_model\n",
        "from datetime import datetime\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "from google.colab import drive, userdata\n",
        "import time\n",
        "import tempfile\n",
        "import threading\n",
        "from pyngrok import ngrok\n",
        "from collections import deque\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, clear_output\n",
        "import traceback\n",
        "import re\n",
        "!ngrok authtoken 2bfZVe89b38P2ucDrS9ncLziKEp_77cvUVKv3h6asfZgdhTvS\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0lIrL69UT2d"
      },
      "source": [
        "#Creating Model Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCRV8D1PUKsW"
      },
      "outputs": [],
      "source": [
        "class Model_builder :\n",
        "  def __init__(self, id, isNew = True) :\n",
        "    print(tf.config.list_physical_devices('GPU'))\n",
        "    tf.keras.utils.set_random_seed(13)\n",
        "    self.optm_vars = {}\n",
        "    if isNew :\n",
        "      self.build_model()\n",
        "      self.logs = pd.DataFrame({'Actor Loss': [],\n",
        "                                'Critic Loss': [],\n",
        "                                'Entropy' : [],\n",
        "                                'Total Loss' : []\n",
        "                              })\n",
        "      self.base_optimizer = optimizers.Adam()\n",
        "      self.actor_optimizer = optimizers.Adam()\n",
        "      self.critic_optimizer = optimizers.Adam()\n",
        "      self.save_model(id)\n",
        "    else :\n",
        "      self.base_model = load_model(os.path.join('/content/gdrive', 'My Drive', 'CHESS-AI', id, 'base_model.keras'))\n",
        "      self.actor_model = load_model(os.path.join('/content/gdrive', 'My Drive', 'CHESS-AI', id, 'actor_model.keras'))\n",
        "      self.critic_model = load_model(os.path.join('/content/gdrive', 'My Drive', 'CHESS-AI', id, 'critic_model.keras'))\n",
        "      self.compile_model()\n",
        "      self.logs = pd.read_csv(os.path.join('/content/gdrive', 'My Drive', 'CHESS-AI', id, 'logs.csv'))\n",
        "      self.base_optimizer = optimizers.Adam()\n",
        "      self.actor_optimizer = optimizers.Adam()\n",
        "      self.critic_optimizer = optimizers.Adam()\n",
        "      #self.base_optimizer = self.load_optimizers(id, \"base\")\n",
        "      #self.actor_optimizer = self.load_optimizers(id, \"actor\")\n",
        "      #self.critic_optimizer = self.load_optimizers(id, \"critic\")\n",
        "\n",
        "  def build_model(self) :\n",
        "    # Designing NN Architecture\n",
        "    inp_base = Input(shape = [8,8,60], name = \"base_input\", dtype = tf.float32)\n",
        "    base1 = Conv2D(256,(3,3), padding = \"same\", dtype = tf.float32, kernel_initializer = tf.keras.initializers.GlorotNormal())(inp_base)\n",
        "    base1 = ELU(alpha=0.1)(base1)\n",
        "    base = BatchNormalization(dtype = tf.float32)(base1)\n",
        "    base = Conv2D(256,(3,3), padding = \"same\", dtype = tf.float32, kernel_initializer = tf.keras.initializers.GlorotNormal())(base)\n",
        "    base = ELU(alpha=0.1)(base)\n",
        "    base = Add(dtype = tf.float32)([base, base1])\n",
        "    base = BatchNormalization(dtype = tf.float32)(base)\n",
        "    base = Conv2D(256,(3,3), padding = \"same\", dtype = tf.float32, kernel_initializer = tf.keras.initializers.GlorotNormal())(base)\n",
        "    base = ELU(alpha=0.1)(base)\n",
        "    base = Add(dtype = tf.float32)([base, base1])\n",
        "    base = BatchNormalization(dtype = tf.float32)(base)\n",
        "    base = Conv2D(256,(3,3), padding = \"same\", dtype = tf.float32, kernel_initializer = tf.keras.initializers.GlorotNormal())(base)\n",
        "    base = ELU(alpha=0.1)(base)\n",
        "    base = Add(dtype = tf.float32)([base, base1])\n",
        "    base = BatchNormalization(dtype = tf.float32)(base)\n",
        "    base = Conv2D(256,(3,3), padding = \"same\", dtype = tf.float32, kernel_initializer = tf.keras.initializers.GlorotNormal())(base)\n",
        "    base = ELU(alpha=0.1)(base)\n",
        "    base = Add(dtype = tf.float32)([base, base1])\n",
        "    base = BatchNormalization(dtype = tf.float32)(base)\n",
        "    base1 = Conv2D(128,(3,3), padding = \"same\", dtype = tf.float32, kernel_initializer = tf.keras.initializers.GlorotNormal())(base)\n",
        "    base1 = ELU(alpha=0.1)(base1)\n",
        "    base = BatchNormalization(dtype = tf.float32)(base1)\n",
        "    base = Conv2D(128,(3,3), padding = \"same\", dtype = tf.float32, kernel_initializer = tf.keras.initializers.GlorotNormal())(base)\n",
        "    base = ELU(alpha=0.1)(base)\n",
        "    base = Add(dtype = tf.float32)([base, base1])\n",
        "    base_head = BatchNormalization(name=\"bn4\", dtype = tf.float32)(base) #8,8,128\n",
        "    self.base_model = Model(inputs= inp_base, outputs=base_head, name = \"base_model\")\n",
        "\n",
        "    inp_value = Input(shape = [8,8,128], name = \"Value_Inp\", dtype = tf.float32)\n",
        "    val = Conv2D(64, (3, 3), padding='same', dtype = tf.float32, kernel_initializer = tf.keras.initializers.GlorotNormal())(inp_value)\n",
        "    val = ELU(alpha=0.1)(val)\n",
        "    val = BatchNormalization(dtype = tf.float64)(val)\n",
        "    val = Flatten()(val)\n",
        "    val1 = Dense(256, dtype = tf.float32, kernel_initializer = tf.keras.initializers.GlorotNormal())(val)\n",
        "    val1 = ELU(alpha=0.1)(val1)\n",
        "    val = BatchNormalization(dtype = tf.float32)(val1)\n",
        "    val = Dense(256, name=\"v1\", dtype = tf.float32, kernel_initializer = tf.keras.initializers.GlorotNormal())(val)\n",
        "    val = ELU(alpha=0.1)(val)\n",
        "    val = Add(dtype = tf.float32)([val, val1])\n",
        "    val = BatchNormalization( name=\"v2\", dtype = tf.float32)(val)\n",
        "    val1 = Dense(128, name=\"v5\", dtype = tf.float32, kernel_initializer = tf.keras.initializers.GlorotNormal())(val)\n",
        "    val1 = ELU(alpha=0.1)(val1)\n",
        "    val = BatchNormalization( name=\"v6\", dtype = tf.float32)(val1)\n",
        "    val = Dense(128, name=\"v7\", dtype = tf.float32, kernel_initializer = tf.keras.initializers.GlorotNormal())(val)\n",
        "    val = ELU(alpha=0.1)(val)\n",
        "    val = Add(dtype = tf.float32)([val, val1])\n",
        "    val = BatchNormalization(name=\"v8\", dtype = tf.float32)(val)\n",
        "    val = Dense(64, name=\"v9\", dtype = tf.float32, kernel_initializer = tf.keras.initializers.GlorotNormal())(val)\n",
        "    val = ELU(alpha=0.1)(val)\n",
        "    val = BatchNormalization( name=\"v10\", dtype = tf.float32)(val)\n",
        "    value_head = Dense(1, activation = \"tanh\", name=\"v_head\", dtype = tf.float64, kernel_initializer = tf.keras.initializers.GlorotNormal())(val)\n",
        "    self.critic_model = Model(inputs= inp_value, outputs=value_head, name = \"critic_model\")\n",
        "\n",
        "    inp_policy = Input(shape = [8,8,128], name = \"Policy_Inp1\", dtype = tf.float32)\n",
        "    moves_mask = Input(shape = [8,8,10], name = \"Policy_Inp2\", dtype = tf.float32)\n",
        "    policy = Conv2D(256, (3, 3), padding='same', dtype = tf.float32, kernel_initializer = tf.keras.initializers.GlorotNormal())(inp_policy)\n",
        "    policy = ELU(alpha=0.1)(policy)\n",
        "    policy = BatchNormalization(dtype = tf.float32)(policy)\n",
        "    policy1 = Conv2D(128, (3, 3), padding='same', dtype = tf.float32, kernel_initializer = tf.keras.initializers.GlorotNormal())(policy)\n",
        "    policy1 = ELU(alpha=0.1)(policy1)\n",
        "    policy = BatchNormalization(dtype = tf.float32)(policy1)\n",
        "    policy = Conv2D(128, (3, 3), padding='same', dtype = tf.float32, kernel_initializer = tf.keras.initializers.GlorotNormal())(policy)\n",
        "    policy = ELU(alpha=0.1)(policy)\n",
        "    policy = Add(dtype = tf.float32)([policy,policy1])\n",
        "    policy = BatchNormalization(dtype = tf.float32)(policy)\n",
        "    policy1 = Conv2D(64, (3, 3), padding='same', dtype = tf.float32, kernel_initializer = tf.keras.initializers.GlorotNormal())(policy)\n",
        "    policy1 = ELU(alpha=0.1)(policy1)\n",
        "    policy = BatchNormalization(dtype = tf.float32)(policy1)\n",
        "    policy = Conv2D(64, (3, 3), padding='same', dtype = tf.float32, kernel_initializer = tf.keras.initializers.GlorotNormal())(policy)\n",
        "    policy = ELU(alpha=0.1)(policy)\n",
        "    policy = Add(dtype = tf.float32)([policy,policy1])\n",
        "    policy = BatchNormalization(dtype = tf.float32)(policy)\n",
        "    policy1 = Conv2D(32, (3, 3), padding='same', dtype = tf.float32, kernel_initializer = tf.keras.initializers.GlorotNormal())(policy)\n",
        "    policy1 = ELU(alpha=0.1)(policy1)\n",
        "    policy = BatchNormalization(dtype = tf.float32)(policy1)\n",
        "    policy = Conv2D(32, (3, 3), padding='same', dtype = tf.float32, kernel_initializer = tf.keras.initializers.GlorotNormal())(policy)\n",
        "    policy = ELU(alpha=0.1)(policy)\n",
        "    policy = Add(dtype = tf.float32)([policy,policy1])\n",
        "    policy = BatchNormalization(dtype = tf.float32)(policy)\n",
        "    policy = Conv2D(10, (3, 3), padding='same', name = \"p19\", dtype = tf.float32, kernel_initializer = tf.keras.initializers.GlorotNormal())(policy)\n",
        "    policy = ELU(alpha=0.1)(policy)\n",
        "    policy = BatchNormalization( name = \"p20\", dtype = tf.float64)(policy)\n",
        "    policy = Multiply(name = \"pmul\", dtype = tf.float64)([policy, moves_mask])\n",
        "    policy = Flatten()(policy)\n",
        "    policy_head = Softmax(name=\"p_head\", dtype = tf.float64)(policy)\n",
        "    self.actor_model = Model(inputs= [inp_policy, moves_mask], outputs=policy_head, name = \"actor_model\")\n",
        "    self.compile_model()\n",
        "\n",
        "  def compile_model(self) :\n",
        "    inp1_merged = Input(shape = [8,8,60], name = \"PositionMask\", dtype = tf.float32)\n",
        "    inp2_merged = Input(shape = [8,8,10], name = \"LegalMovesMask\", dtype = tf.float32)\n",
        "    value_output = self.critic_model(self.base_model(inp1_merged))\n",
        "    policy_output = self.actor_model([self.base_model(inp1_merged), inp2_merged])\n",
        "    self.model = Model(inputs= (inp1_merged, inp2_merged), outputs=(value_output, policy_output))\n",
        "\n",
        "  def update_model(self, inputs, masks, values, actions, id, config) :\n",
        "    print(\"Model Update Begins\")\n",
        "    tf.keras.utils.set_random_seed(13)\n",
        "    self.base_optimizer.learning_rate.assign(config[\"base_learning_rate\"])\n",
        "    self.actor_optimizer.learning_rate.assign(config[\"actor_learning_rate\"])\n",
        "    self.critic_optimizer.learning_rate.assign(config[\"critic_learning_rate\"])\n",
        "    epochs = 12\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((inputs, masks, values, actions))\n",
        "    for epoch in range(epochs) :\n",
        "      train_dataset_batches = train_dataset.shuffle(buffer_size=32000).batch(config[\"batch_size\"])\n",
        "      for step, (inputs_batch, masks_batch, values_batch, actions_batch) in enumerate(train_dataset_batches) :\n",
        "        try :\n",
        "          with tf.GradientTape() as base_tape, tf.GradientTape() as actor_tape, tf.GradientTape() as critic_tape:\n",
        "            base_out = self.base_model(inputs = inputs_batch, training = True)\n",
        "            value_predictions = self.critic_model(inputs = base_out, training = True)\n",
        "            action_probs = self.actor_model(inputs = (base_out, masks_batch), training = True)\n",
        "            actor_loss, critic_loss, total_loss = self.compute_loss(values_batch, tf.squeeze(value_predictions), action_probs, actions_batch, config)\n",
        "\n",
        "          gradients_base = base_tape.gradient(total_loss, self.base_model.trainable_variables)\n",
        "          gradients_actor = actor_tape.gradient(actor_loss, self.actor_model.trainable_variables)\n",
        "          gradients_critic = critic_tape.gradient(critic_loss, self.critic_model.trainable_variables)\n",
        "\n",
        "          self.base_optimizer.apply_gradients(zip(gradients_base, self.base_model.trainable_variables))\n",
        "          self.actor_optimizer.apply_gradients(zip(gradients_actor, self.actor_model.trainable_variables))\n",
        "          self.critic_optimizer.apply_gradients(zip(gradients_critic, self.critic_model.trainable_variables))\n",
        "\n",
        "          print((\"Epochs -\", epoch+1, \"/\", epochs, \"step -\", step, \"batch_loss -\",tf.get_static_value(total_loss)))\n",
        "          sys.stdout.flush()\n",
        "        except Exception:\n",
        "          print(traceback.print_exc())\n",
        "    del train_dataset\n",
        "    self.compile_model()\n",
        "\n",
        "  def compute_loss(self, returns, value_predictions, action_probs, actions, config):\n",
        "    advantages = tf.subtract(tf.cast(returns, dtype=tf.float64), value_predictions)\n",
        "    critic_loss = tf.losses.mean_squared_error(returns, value_predictions)\n",
        "    action_log_probs = tf.math.log(action_probs + 1e-10)\n",
        "    entropy = tf.reduce_mean(tf.reduce_sum(action_probs * action_log_probs, axis = 1))\n",
        "    selected_action_log_probs = tf.reduce_sum(action_log_probs * tf.cast(actions, dtype=tf.float64), axis = 1)\n",
        "    actor_loss = tf.reduce_mean(tf.multiply(selected_action_log_probs, advantages))\n",
        "    total_loss = config[\"actor_coefficient\"] * actor_loss + config[\"critic_coefficient\"] * critic_loss - config[\"entropy_coefficient\"] * entropy\n",
        "\n",
        "    self.logs = pd.concat([self.logs, pd.DataFrame({'Actor Loss': [tf.get_static_value(actor_loss)],\n",
        "                                'Critic Loss': [tf.get_static_value(critic_loss)],\n",
        "                                'Entropy' : [tf.get_static_value(entropy)],\n",
        "                                'Total Loss' : [tf.get_static_value(total_loss)]\n",
        "                              })], ignore_index= True)\n",
        "    self.logs.reset_index(drop=True, inplace=True)\n",
        "    return ((actor_loss - config[\"entropy_coefficient\"] * entropy), critic_loss, total_loss)\n",
        "\n",
        "  def get_piece_to_value(self, color, inp = True) :\n",
        "    if inp :\n",
        "      if(color == 1) :\n",
        "        return {\n",
        "        'P': 0, 'N': 1, 'B': 2, 'R': 3, 'Q': 4, 'K': 5,\n",
        "        'p': 6, 'n':7, 'b': 8, 'r': 9, 'q': 10, 'k': 11\n",
        "        }\n",
        "      return {\n",
        "        'p': 0, 'n': 1, 'b': 2, 'r': 3, 'q': 4, 'k': 5,\n",
        "        'P': 6, 'N':7, 'B': 8, 'R': 9, 'Q': 10, 'K': 11\n",
        "      }\n",
        "    else :\n",
        "      if(color == 1) :\n",
        "        return {\n",
        "        'P': 0, 'N': 1, 'B': 3, 'R': 5, 'Q': 7, 'K': 8\n",
        "        }\n",
        "      return {\n",
        "        'p': 0, 'n': 1, 'b': 3, 'r': 5, 'q': 7, 'k': 8\n",
        "      }\n",
        "\n",
        "  def load_optimizers(self, id, name) :\n",
        "    optm = optimizers.Adam()\n",
        "    optm._variables = []\n",
        "    with open(os.path.join('/content/gdrive', 'My Drive', 'CHESS-AI', id, name+'_optimizer_weights.json'), 'r') as file:\n",
        "      json_string = file.read()\n",
        "    optm_dict = json.loads(json_string)\n",
        "    for i in range(len(optm_dict[\"data\"])):\n",
        "      optm.add_variable(optm_dict[\"data\"][i][\"shape\"], tf.dtypes.as_dtype(optm_dict[\"data\"][i][\"dtype\"]), name = optm_dict[\"data\"][i][\"name\"])\n",
        "      optm.variables[i].assign(optm_dict[\"data\"][i][\"value\"])\n",
        "    self.optm_vars[name] = len(optm_dict[\"data\"])\n",
        "    return optm\n",
        "\n",
        "  def save_model(self, id) :\n",
        "    print(\"Model Saving Process Initiated\")\n",
        "    directory = os.path.join('/content/gdrive', 'My Drive', 'CHESS-AI', id, \"Exploration_Backup\")\n",
        "    if not os.path.exists(directory):\n",
        "      os.makedirs(directory)\n",
        "    self.model.save(os.path.join('/content/gdrive', 'My Drive', 'CHESS-AI', id, 'model.h5'), save_format = \"h5\")\n",
        "    self.model.save(os.path.join('/content/gdrive', 'My Drive', 'CHESS-AI', id, 'model.keras'), save_format = \"keras\")\n",
        "    self.base_model.save(os.path.join('/content/gdrive', 'My Drive', 'CHESS-AI', id, 'base_model.keras'), save_format = \"keras\")\n",
        "    self.actor_model.save(os.path.join('/content/gdrive', 'My Drive', 'CHESS-AI', id, 'actor_model.keras'), save_format = \"keras\")\n",
        "    self.critic_model.save(os.path.join('/content/gdrive', 'My Drive', 'CHESS-AI', id, 'critic_model.keras'), save_format = \"keras\")\n",
        "    self.logs.to_csv(os.path.join('/content/gdrive', 'My Drive', 'CHESS-AI', id, 'logs.csv'))\n",
        "    self.save_optimizers(id, self.base_optimizer, \"base\")\n",
        "    self.save_optimizers(id, self.actor_optimizer, \"actor\")\n",
        "    self.save_optimizers(id, self.critic_optimizer, \"critic\")\n",
        "    print(\"Model Saving Process Completed\")\n",
        "\n",
        "  def save_optimizers(self, id, optimizer, name) :\n",
        "    optm_dict = {\n",
        "        \"data\" : []\n",
        "    }\n",
        "    for v in optimizer.variables :\n",
        "      optm_dict[\"data\"].append({\n",
        "          \"shape\" : v.shape.as_list(),\n",
        "          \"dtype\" : re.split(pattern=\"(?::|')\", string = str(v.dtype))[2],\n",
        "          \"name\" : v.name.split(\":\")[0],\n",
        "          \"value\" : v.numpy().tolist()\n",
        "      })\n",
        "    with open(os.path.join('/content/gdrive', 'My Drive', 'CHESS-AI', id, name+'_optimizer_weights.json'), 'w') as drive_file:\n",
        "      drive_file.write(json.dumps(optm_dict, indent=2))\n",
        "    print((\"original optimisers vars length\", self.optm_vars[name], \"new optimisers vars length\", len(optm_dict[\"data\"])))\n",
        "    self.optm_vars[name] = len(optm_dict[\"data\"])\n",
        "\n",
        "  def get_model_summary(self,visualize = True) :\n",
        "    if(visualize) :\n",
        "      return plot_model(self.model,show_shapes = True, show_dtype=True, expand_nested=True)\n",
        "    else :\n",
        "      return self.model.summary()\n",
        "#Model_builder(\"vjvu\").get_model_summary(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRzY4R1zU5O8"
      },
      "source": [
        "#Setting Up Server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krmasnvJZW0t"
      },
      "outputs": [],
      "source": [
        "model = None\n",
        "queue = None\n",
        "completed_updates = None\n",
        "app= Flask(__name__)\n",
        "run_with_ngrok(app)\n",
        "\n",
        "@app.route(\"/ready-model/<id>\", methods = [\"GET\"])\n",
        "def load(id):\n",
        "  global model\n",
        "  if id == \"None\" :\n",
        "    id = datetime.now().strftime(\"%d_%m_%Y-%H_%M_%S\")\n",
        "    model[id] = Model_builder(id)\n",
        "    print(\"Created Model with id - \" + id)\n",
        "  else :\n",
        "    if id in model :\n",
        "      print(f\"Model with id {id} is already loaded and ready to be used\")\n",
        "      return id\n",
        "    model[id]= Model_builder(id, False)\n",
        "    print(\"Loaded Model with id - \" + id)\n",
        "  return id\n",
        "\n",
        "@app.route(\"/download-model/<id>\", methods = [\"GET\"])\n",
        "def return_model(id):\n",
        "  temp_file = tempfile.NamedTemporaryFile(delete=False, suffix = \".keras\")\n",
        "  model[id].model.save(temp_file.name, save_format = \"keras\")\n",
        "  temp_file.close()\n",
        "  return send_file(temp_file.name, as_attachment=True)\n",
        "\n",
        "@app.route(\"/train/<id>\", methods = [\"POST\"])\n",
        "def update(id) :\n",
        "  global model\n",
        "  if id not in model :\n",
        "    return \"Error\", 400\n",
        "  global queue\n",
        "  data = request.get_json()\n",
        "  ack_id = datetime.now().strftime(\"%H_%M_%S_%f\")\n",
        "  queue.append((id,data[\"inputs\"], data[\"masks\"], data[\"values\"], data[\"actions\"], data[\"config\"], ack_id))\n",
        "  print((\"Appending Queue... Final size is - \", len(queue)))\n",
        "  return ack_id\n",
        "\n",
        "@app.route(\"/check-update-status/<ack_id>\", methods = [\"GET\"])\n",
        "def check(ack_id):\n",
        "  global completed_updates\n",
        "  if ack_id in completed_updates :\n",
        "    completed_updates.remove(ack_id)\n",
        "    return \"Done\"\n",
        "  return \"Not Done\"\n",
        "\n",
        "@app.route(\"/save-model/<id>\", methods = [\"GET\"])\n",
        "def save(id):\n",
        "  model[id].save_model(id)\n",
        "  return \"Model Saved Successfully\"\n",
        "\n",
        "def process_update() :\n",
        "  global queue\n",
        "  global model\n",
        "  global completed_updates\n",
        "  model = {}\n",
        "  queue = deque()\n",
        "  completed_updates = set()\n",
        "  while True:\n",
        "    if queue:\n",
        "      data = queue.popleft()\n",
        "      print(\"Starting Update for ID - \"+ data[0])\n",
        "      model[data[0]].update_model(data[1], data[2], data[3], data[4], data[0], data[5])\n",
        "      completed_updates.add(data[6])\n",
        "      if(len(queue)==0) :\n",
        "        model[data[0]].save_model(data[0])\n",
        "\n",
        "    else :\n",
        "      #print(\"Nothing to Update\")\n",
        "      time.sleep(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXjc0qqXyKFj"
      },
      "source": [
        "#Setting Up Live Plotting\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-gI8O1gvT0B"
      },
      "outputs": [],
      "source": [
        "# @title Live Plotting Code {display-mode: \"form\"}\n",
        "plot_output = widgets.Output(layout={'border': '1px solid black'})\n",
        "display(widgets.VBox([widgets.HTML(value=\"<b>Live Plot:</b>\"), plot_output]))\n",
        "\n",
        "def plot_metrics(model) :\n",
        "  clear_output(wait = True)\n",
        "  count = 0\n",
        "  fig = plt.figure(figsize = (16,8))\n",
        "  while True :\n",
        "    if(len(model) != 0) :\n",
        "      if count == len(model) :\n",
        "        update_plots(model, fig)\n",
        "      else :\n",
        "        count = draw_plots(model, fig)\n",
        "    time.sleep(4)\n",
        "\n",
        "def draw_plots(model, fig) :\n",
        "  fig.clf()\n",
        "  with plot_output:\n",
        "    clear_output(wait = True)\n",
        "    keys = list(model.keys())\n",
        "    for i in range(len(model)) :\n",
        "      ax = fig.add_subplot(1, len(model), i+1)\n",
        "      if not model[keys[i]].logs.empty :\n",
        "        ax.clear()\n",
        "        ax.plot(model[keys[i]].logs[\"Actor Loss\"], label = \"Actor Loss\")\n",
        "        ax.plot(model[keys[i]].logs[\"Critic Loss\"], label = \"Critic Loss\")\n",
        "        ax.plot(model[keys[i]].logs[\"Entropy\"], label = \"Entropy\")\n",
        "        ax.plot(model[keys[i]].logs[\"Total Loss\"], label = \"Total Loss\")\n",
        "        ax.legend(loc = 'upper right')\n",
        "    display(fig)\n",
        "  return len(model)\n",
        "def update_plots(model, fig) :\n",
        "  with plot_output:\n",
        "    clear_output(wait = True)\n",
        "    keys = list(model.keys())\n",
        "    for i, ax in enumerate(fig.get_axes()):\n",
        "      if not model[keys[i]].logs.empty :\n",
        "        ax.clear()\n",
        "        ax.plot(model[keys[i]].logs[\"Actor Loss\"], label = \"Actor Loss\")\n",
        "        ax.plot(model[keys[i]].logs[\"Critic Loss\"], label = \"Critic Loss\")\n",
        "        ax.plot(model[keys[i]].logs[\"Entropy\"], label = \"Entropy\")\n",
        "        ax.plot(model[keys[i]].logs[\"Total Loss\"], label = \"Total Loss\")\n",
        "        ax.legend(loc = 'upper right')\n",
        "    display(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzkgILHOA7MW"
      },
      "source": [
        "#Running Server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lHF237ZO1AK"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  threading.Thread(target=process_update, daemon = True).start()\n",
        "  #threading.Thread(target=plot_metrics, args = [model], daemon = True).start()\n",
        "  url = ngrok.connect(5000).public_url\n",
        "  with open(os.path.join('/content/gdrive', 'My Drive', 'CHESS-AI', 'url.txt'), 'w') as file:\n",
        "    file.write(url)\n",
        "  app.run()\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "    ngrok.kill()\n",
        "    del model\n",
        "    del completed_updates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kZuuWcmgEYx"
      },
      "outputs": [],
      "source": [
        "ngrok.kill()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "w3m8kVQWUMyD",
        "D0lIrL69UT2d",
        "FRzY4R1zU5O8",
        "VXjc0qqXyKFj"
      ],
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}