{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFPsLk4FLxsq"
      },
      "source": [
        "# Deep Reinforcement Learning based Chess Bot\n",
        "## The bot will learn chess from scratch solely by playing against itself\n",
        "\n",
        "\n",
        "## <center>Let's Begin The Journey !!!</center>\n",
        "<center>ðŸ•‰ðŸ•‰ðŸ•‰ðŸ•‰ðŸ•‰ðŸ•‰ðŸ•‰ðŸ•‰ðŸ•‰ðŸ•‰ðŸ•‰ðŸ•‰ðŸ•‰ðŸ•‰ðŸ•‰ðŸ•‰ðŸ•‰ðŸ•‰ðŸ•‰ðŸ•‰ðŸ•‰ðŸ•‰ðŸ•‰ðŸ•‰ðŸ•‰ðŸ•‰</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HcVW0_yRLuR"
      },
      "source": [
        "#Importing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtmqN-44h5NJ"
      },
      "outputs": [],
      "source": [
        "!pip install python-chess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPepTka5iNZe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import chess\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from keras import Model, optimizers\n",
        "from keras.models import load_model\n",
        "from keras.layers import *\n",
        "from keras.utils import plot_model\n",
        "import sys\n",
        "import os\n",
        "import gc\n",
        "import json\n",
        "from google.colab import drive\n",
        "from collections import deque\n",
        "import multiprocessing\n",
        "import threading\n",
        "import requests\n",
        "import time\n",
        "import tempfile\n",
        "import traceback\n",
        "import warnings\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDMwvNqdoQPB"
      },
      "source": [
        "#Creating Model Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmSUsI36QLeh"
      },
      "outputs": [],
      "source": [
        "class Model_handler :\n",
        "  def __init__(self, id = \"None\") :\n",
        "    self.server_url = self.get_server_url()\n",
        "    self.session = requests.Session()\n",
        "    self.id =  self.send_request(\"/ready-model/\", id, data = None)\n",
        "    self.id = self.id.text\n",
        "    self.is_updated_version = False\n",
        "\n",
        "  def load_latest(self) :\n",
        "    response = self.send_request(\"/download-model/\", self.id, data = None)\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".keras\") as temp_model_file:\n",
        "      temp_model_file.write(response.content)\n",
        "    self.model = tf.keras.models.load_model(temp_model_file.name, compile = False)\n",
        "    self.is_updated_version = True\n",
        "\n",
        "  def check_update_status(self, acknowledge_id) :\n",
        "    response = self.send_request(\"/check-update-status/\", acknowledge_id, data = None)\n",
        "    if response.text == \"Done\" :\n",
        "      return True\n",
        "    return False\n",
        "\n",
        "  def predict(self, state, color) : #as per the network\n",
        "    prediction = self.model(self.convert_state_to_input(state, color))\n",
        "    return self.convert_output_to_probs(state, color, prediction[0].numpy()[0][0], prediction[1][0])\n",
        "\n",
        "  def bulk_predict(self, arr) :\n",
        "    states = [x[0].state for x in arr]\n",
        "    colors = [x[0].color for x in arr]\n",
        "    batch_size = 64\n",
        "    inputs, masks = self.convert_state_to_input(states, colors)\n",
        "    for i in range(0, len(states), batch_size):\n",
        "      value_predictions, _ = self.model(inputs = (inputs[i:i+batch_size], masks[i:i+batch_size]))\n",
        "      for j in range(0, len(value_predictions)) :\n",
        "        arr[i+j][0].value = value_predictions.numpy()[j][0]\n",
        "\n",
        "  def update_model(self, states, colors, values, actions,config) :\n",
        "    println(\"Model Update\")\n",
        "    inputs, masks = self.convert_state_to_input(states, colors)\n",
        "    result = self.send_request(\"/train/\", self.id, data = {\n",
        "        \"inputs\": inputs.tolist(),\n",
        "        \"masks\": masks.tolist(),\n",
        "        \"values\": values,\n",
        "        \"actions\": actions,\n",
        "        \"config\": config\n",
        "      })\n",
        "    self.is_updated_version = False\n",
        "    return result.text\n",
        "\n",
        "  def get_server_url(self) :\n",
        "    with open(os.path.join('/content/gdrive', 'My Drive', 'CHESS-AI', 'url.txt'), 'r') as drive_file :\n",
        "      content = drive_file.read()\n",
        "    return content\n",
        "\n",
        "  def send_request(self, path, id, data = None) :\n",
        "    first_try = True\n",
        "    while True :\n",
        "      try :\n",
        "        if data is None :\n",
        "          response = self.session.get(url = self.server_url + path + id)\n",
        "        else :\n",
        "          response = self.session.post(url = self.server_url + path + id, json = data)\n",
        "        if response.status_code == 200:\n",
        "          return response\n",
        "        while True :\n",
        "          self.server_url = self.get_server_url()\n",
        "          println(\"Retrying request with URL \"+self.server_url)\n",
        "          if path == \"/ready-model/\" :\n",
        "            temp = self.session.get(url = self.server_url+\"/ready-model/\"+id)\n",
        "            if temp.status_code == 200 :\n",
        "              return temp\n",
        "          else :\n",
        "            temp = self.session.get(url = self.server_url+\"/ready-model/\"+self.id)\n",
        "            if temp.status_code == 200 :\n",
        "              return self.send_request(path, id, data)\n",
        "          time.sleep(10)\n",
        "      except Exception as e:\n",
        "        if first_try :\n",
        "          first_try = False\n",
        "        print(e)\n",
        "        self.session = requests.Session()\n",
        "        self.server_url = self.get_server_url()\n",
        "\n",
        "  def convert_state_to_input(self, state, color) :\n",
        "    if type(state) == str :\n",
        "      temp = state.split(\"_\")\n",
        "      temp_arr = np.zeros((8,8,12), dtype = np.float16)\n",
        "      arr2 = np.zeros((8,8,10), dtype = np.float16)\n",
        "      temp_arr, arr2 = self.convert_board_to_input(temp[-1], color)\n",
        "      for i in range(1,5) :\n",
        "        temp_arr = np.concatenate([self.convert_board_to_input(temp[-1-i], color, False), temp_arr], axis = 2)\n",
        "      return (np.expand_dims(temp_arr, axis = 0), np.expand_dims(arr2, axis = 0))\n",
        "    inputs = np.empty(shape = (len(state),8,8,60))\n",
        "    masks = np.empty(shape = (len(state),8,8,10))\n",
        "    for s in range(len(state)) :\n",
        "      temp = state[s].split(\"_\")\n",
        "      temp_arr = np.zeros((8,8,12), dtype = np.float16)\n",
        "      arr2 = np.zeros((8,8,10), dtype = np.float16)\n",
        "      temp_arr, arr2 = self.convert_board_to_input(temp[-1], color[s])\n",
        "      for i in range(1,5) :\n",
        "        temp_arr = np.concatenate([self.convert_board_to_input(temp[-1-i], color[s], False), temp_arr], axis = 2)\n",
        "      inputs[s] = temp_arr\n",
        "      masks[s] = arr2\n",
        "    return (inputs, masks)\n",
        "\n",
        "  def convert_board_to_input(self, state, color, current = True) :\n",
        "    if current :\n",
        "      board = chess.Board(state)\n",
        "      board.turn = color\n",
        "      arr = np.zeros((8,8,12), dtype = np.float16)\n",
        "      arr2 = np.zeros((8,8,10), dtype = np.float16)\n",
        "      piece_to_value = self.get_piece_to_value(color)\n",
        "      piece_to_value2 = self.get_piece_to_value(color, False)\n",
        "      for i in range(64) :\n",
        "        if(board.piece_at(i) is not None) :\n",
        "          arr[i//8,i%8,piece_to_value[board.piece_at(i).symbol()]] = 1\n",
        "      for move in board.legal_moves :\n",
        "        square = move.to_square\n",
        "        arr[square//8, square%8, piece_to_value[board.piece_at(move.from_square).symbol()]] = 0.5\n",
        "        symbol = board.piece_at(move.from_square).symbol()\n",
        "        if move.promotion is not None :\n",
        "          arr2[move.promotion-2, move.from_square%8, 9] = 1\n",
        "        else :\n",
        "          arr2[square//8, square%8, piece_to_value2[symbol]] = 1\n",
        "          if(piece_to_value2[symbol] == 1 or piece_to_value2[symbol] == 3 or piece_to_value2[symbol] == 5) :\n",
        "            piece_to_value2[symbol] += 1\n",
        "      return (arr, arr2)\n",
        "    else :\n",
        "      arr = np.zeros((8,8,12), dtype = np.float16)\n",
        "      if len(state) == 0 :\n",
        "        return arr\n",
        "      board = chess.Board(state)\n",
        "      board.turn = color\n",
        "      piece_to_value = self.get_piece_to_value(color)\n",
        "      for i in range(64) :\n",
        "        if(board.piece_at(i) is not None) :\n",
        "          arr[i//8,i%8,piece_to_value[board.piece_at(i).symbol()]] = 1\n",
        "      return arr\n",
        "\n",
        "  def get_piece_to_value(self, color, inp = True) :\n",
        "    if inp :\n",
        "      if(color == 1) :\n",
        "        return {\n",
        "        'P': 0, 'N': 1, 'B': 2, 'R': 3, 'Q': 4, 'K': 5,\n",
        "        'p': 6, 'n':7, 'b': 8, 'r': 9, 'q': 10, 'k': 11\n",
        "        }\n",
        "      return {\n",
        "        'p': 0, 'n': 1, 'b': 2, 'r': 3, 'q': 4, 'k': 5,\n",
        "        'P': 6, 'N':7, 'B': 8, 'R': 9, 'Q': 10, 'K': 11\n",
        "      }\n",
        "    else :\n",
        "      if(color == 1) :\n",
        "        return {\n",
        "        'P': 0, 'N': 1, 'B': 3, 'R': 5, 'Q': 7, 'K': 8\n",
        "        }\n",
        "      return {\n",
        "        'p': 0, 'n': 1, 'b': 3, 'r': 5, 'q': 7, 'k': 8\n",
        "      }\n",
        "\n",
        "  def convert_action_to_softmax(self, states, colors, moves) :\n",
        "    array = []\n",
        "    for i in range(len(states)) :\n",
        "      board = chess.Board(states[i].split(\"_\")[-1])\n",
        "      board.turn = colors[i]\n",
        "      move = chess.Move.from_uci(moves[i])\n",
        "      to_square = move.to_square\n",
        "      from_square = move.from_square\n",
        "      piece_to_value = self.get_piece_to_value(colors[i],False)\n",
        "      piece_type = piece_to_value[board.piece_at(from_square).symbol()]\n",
        "      arr = np.zeros((8,8,10), dtype = np.float64)\n",
        "      if(piece_type == 1 or piece_type == 3 or piece_type == 5) :\n",
        "        present = False\n",
        "        for i in range(from_square) :\n",
        "          if board.piece_at(i) is not None :\n",
        "            if board.piece_at(i).symbol() == board.piece_at(from_square).symbol() :\n",
        "              arr[to_square//8, to_square%8, piece_type+1] = 1\n",
        "              present = True\n",
        "              break\n",
        "        if not present :\n",
        "          arr[to_square//8, to_square%8, piece_type] = 1\n",
        "      else :\n",
        "        if move.promotion is not None :\n",
        "          arr[move.promotion-2, from_square%8, 9] = 1\n",
        "        else :\n",
        "          arr[to_square//8, to_square%8, piece_type] = 1\n",
        "      array.append(arr.flatten().tolist())\n",
        "    return array\n",
        "\n",
        "  def convert_output_to_probs(self, state, color, value_output, policy_output) :\n",
        "    policy = tf.reshape(policy_output, [8,8,10])\n",
        "    board = chess.Board(state.split(\"_\")[-1])\n",
        "    board.turn = color\n",
        "    piece_to_value = self.get_piece_to_value(color,False)\n",
        "    move_dict = {}\n",
        "    for move in list(board.legal_moves) :\n",
        "      to_square = move.to_square\n",
        "      from_square = move.from_square\n",
        "      piece_type = piece_to_value[board.piece_at(from_square).symbol()]\n",
        "      if move.promotion is not None :\n",
        "        move_dict[move.uci()] = policy[move.promotion-2, from_square%8, 9]\n",
        "      else :\n",
        "        move_dict[move.uci()] = policy[to_square//8, to_square%8, piece_type]\n",
        "        if(piece_type == 1 or piece_type == 3 or piece_type == 5) :\n",
        "          piece_to_value[board.piece_at(from_square).symbol()] += 1\n",
        "    return (value_output.item(), [item[0] for item in sorted(move_dict.items(), key = lambda x: x[1], reverse = True)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueDps8HMoET5"
      },
      "source": [
        "# Creating Environment Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VzWMG4sSoH0"
      },
      "outputs": [],
      "source": [
        "class Env :\n",
        "  def __init__(self,board = chess.Board(), turn = True) :\n",
        "    if not (type(board) == str) :\n",
        "      self.board = board\n",
        "      self.game_on = True\n",
        "      self.history = deque([\"\", \"\", \"\", \"\"], maxlen = 4)\n",
        "    else :\n",
        "      self.game_on = True\n",
        "      self.history = deque([\"\", \"\", \"\", \"\"], maxlen = 4)\n",
        "      states = board.split(\"_\")\n",
        "      for i in range(4):\n",
        "        if(len(states[i])>0) :\n",
        "          self.history.append(states[i])\n",
        "      if(len(states[-1])>0) :\n",
        "        self.board = chess.Board(states[-1])\n",
        "      else :\n",
        "        self.board = chess.Board()\n",
        "    self.turn = turn\n",
        "    self.board.turn = turn\n",
        "    self.repeat_penalty_w = 0\n",
        "    self.repeat_penalty_b = 0\n",
        "\n",
        "  def reset(self,agent_w,agent_b) :\n",
        "    self.board = chess.Board()\n",
        "    self.game_on = True\n",
        "    self.history.clear()\n",
        "    agent_w.episodes = []\n",
        "    agent_b.episodes = []\n",
        "\n",
        "  def print(self) :\n",
        "    display(self.board)\n",
        "\n",
        "  def get_legal_moves(self, is_uci = False) :\n",
        "    if(is_uci) :\n",
        "      arr = []\n",
        "      for move in self.board.legal_moves :\n",
        "        arr.append(move.uci())\n",
        "      return arr\n",
        "    return list(self.board.legal_moves)\n",
        "\n",
        "  def get_curr_state(self) :\n",
        "    #return current state\n",
        "    c = 4\n",
        "    prev_state = \"\"\n",
        "    for state in self.history :\n",
        "      prev_state += state + \"_\"\n",
        "    return prev_state + self.board.board_fen()\n",
        "  def do_action(self,action,agent_color) :\n",
        "    #perform action and return immediate reward\n",
        "    self.history.append(self.board.board_fen())\n",
        "    map = self.board.piece_map()\n",
        "    self.board.push_uci(action)\n",
        "    self.turn = (agent_color == False)\n",
        "    return (self.get_immediate_reward(agent_color, map), self.get_curr_state())\n",
        "\n",
        "  def get_immediate_reward(self,color, map) :\n",
        "    outcome = self.board.outcome(claim_draw = True)\n",
        "    if(outcome is None) :\n",
        "      return self.evaluation_fx(color, map)\n",
        "    println(outcome.termination)\n",
        "    self.game_on = False\n",
        "    winner = outcome.winner\n",
        "    if(winner is None) :\n",
        "      return 0\n",
        "    if winner == color :\n",
        "      return 1\n",
        "    return -1\n",
        "\n",
        "  def evaluation_fx(self, color, map) :\n",
        "    board = chess.Board(self.board.board_fen())\n",
        "    board.turn = color\n",
        "    new_map = self.board.piece_map()  ## reward for capturing\n",
        "    if len(map)==len(new_map) :\n",
        "      capture = 0\n",
        "    else :\n",
        "      piece_value = [1, 3, 3, 5, 9, 0]\n",
        "      for piece in map.values() :\n",
        "        if piece not in new_map :\n",
        "          capture = piece_value[piece.piece_type-1]/10\n",
        "          break\n",
        "    moves = [0,0,0,0,0]  ## reward for piece positions\n",
        "    for move in board.legal_moves :\n",
        "      piece = board.piece_type_at(move.from_square)-1\n",
        "      if(piece<5) :\n",
        "        moves[piece] += 1\n",
        "    prev_moves = self.board.move_stack\n",
        "    if len(prev_moves)>=3 :\n",
        "      if prev_moves[-3].from_square==prev_moves[-1].to_square and prev_moves[-1].from_square==prev_moves[-3].to_square :\n",
        "        if color :\n",
        "          self.repeat_penalty_w += 0.35\n",
        "        else :\n",
        "          self.repeat_penalty_b += 0.35\n",
        "      elif prev_moves[-3].to_square==prev_moves[-1].from_square :\n",
        "        if color :\n",
        "          self.repeat_penalty_w += 0.32\n",
        "        else :\n",
        "          self.repeat_penalty_b += 0.32\n",
        "      else :\n",
        "        if color :\n",
        "          self.repeat_penalty_w = 0\n",
        "        else :\n",
        "          self.repeat_penalty_b = 0\n",
        "\n",
        "    if color :\n",
        "      return (moves[0]/16 + moves[1]/15 + moves[2]/24 + moves[3]/26 + moves[4]/24)/4+capture-self.repeat_penalty_w\n",
        "    else :\n",
        "      return (moves[0]/16 + moves[1]/15 + moves[2]/24 + moves[3]/26 + moves[4]/24)/4+capture-self.repeat_penalty_b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHut_fIFoNap"
      },
      "source": [
        "#Creating Agent Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYrVeLaASqaS"
      },
      "outputs": [],
      "source": [
        "class Agent :\n",
        "  def __init__(self,config, model, color) :\n",
        "    self.config = config\n",
        "    self.model = model\n",
        "    self.color = color\n",
        "    self.episodes = []\n",
        " #state,self.env.turn, i, step, start_iter, experience, saved_state, self.rand_gen, self.seed\n",
        "  def get_action(self, state, color, ep_no, step_no, start_iter, experience, saved_tree, seed, logs) :\n",
        "    try :\n",
        "      tree = TD_Tree_Search(self.model, self, state, color, ep_no, step_no, start_iter, experience, seed)\n",
        "      experience = tree.traverse(saved_tree)\n",
        "    except Exception :\n",
        "      print(traceback.print_exc())\n",
        "      time.wait(1000) # to avoid accidently sending wrong data for training\n",
        "    states = []\n",
        "    colors = []\n",
        "    values = []\n",
        "    moves = []\n",
        "    for key in experience.keys() :\n",
        "      data = key.split(\"-\")\n",
        "      states.append(data[0])\n",
        "      colors.append(eval(data[1]))\n",
        "      moves.append(data[2])\n",
        "      values.append(experience[key])\n",
        "      logs = pd.concat([logs, pd.DataFrame({'Step_No': [step_no],\n",
        "                                            'States': [states[-1]],\n",
        "                                            'Colors': [colors[-1]],\n",
        "                                            'Moves': [moves[-1]],\n",
        "                                            'Values' : [values[-1]],\n",
        "                              })], ignore_index= True)\n",
        "      logs.reset_index(drop=True, inplace=True)\n",
        "    acknowlegement_id = self.update_model(states, colors, values, self.model.convert_action_to_softmax(states, colors, moves))\n",
        "    println((\"Update Request sent with acknowledgement No. -\", acknowlegement_id))\n",
        "    logs.to_csv(os.path.join('/content/gdrive', 'My Drive', 'CHESS-AI', self.model.id, \"Exploration_Backup\", \"logs_\"+str(seed)+\".csv\"))\n",
        "    while True :\n",
        "      if self.model.check_update_status(acknowlegement_id) :\n",
        "        break\n",
        "      time.sleep(10)\n",
        "    self.model.load_latest()\n",
        "    println(\"Model Update completed\")\n",
        "    return tree.choose_actions(self.model.predict(state, color)[1],self.config[\"exploit_epsilon\"])\n",
        "\n",
        "  def update_model(self, states, colors, values, actions) :\n",
        "    return self.model.update_model(states, colors, values, actions, self.config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFozK6bQoY2Y"
      },
      "source": [
        "#Creating Trainer Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baX4p4OmXvUC"
      },
      "outputs": [],
      "source": [
        "class Trainer :\n",
        "  def __init__(self, agent_w, agent_b, n_episodes, seed, init_board = chess.Board(), init_turn = True) :\n",
        "    self.init_board = init_board\n",
        "    self.agent_w = agent_w\n",
        "    self.agent_b = agent_b\n",
        "    self.n_eps = n_episodes\n",
        "    self.init_turn = init_turn\n",
        "    self.seed = seed\n",
        "    self.logs = pd.DataFrame({'Step_No': [],\n",
        "                                'States': [],\n",
        "                                'Colors': [],\n",
        "                                'Moves': [],\n",
        "                                'Values' : [],\n",
        "                              })\n",
        "    self.start_training()\n",
        "\n",
        "  def start_training(self) :\n",
        "    if self.agent_w.model.id == self.agent_w.model.id :\n",
        "      self.model = self.agent_w.model\n",
        "      self.id = self.agent_w.model.id\n",
        "    else :\n",
        "      println(\"Error: Different Model IDs used for both agent use single model for both\")\n",
        "      return\n",
        "    println(\"Training Initiated\")\n",
        "    start_eps, start_step, start_iter, experience, tree, init_state, init_turn = self.load_backup_experiences()\n",
        "    for i in range(start_eps,self.n_eps) :\n",
        "      self.env = Env(init_state, init_turn)\n",
        "      step = start_step\n",
        "      while(self.env.game_on) :\n",
        "        state = self.env.get_curr_state()\n",
        "        if self.env.turn :\n",
        "          action = self.agent_w.get_action(state,self.env.turn, i, step, start_iter, experience, tree, self.seed, self.logs)\n",
        "        else :\n",
        "          action = self.agent_b.get_action(state,self.env.turn, i, step, start_iter, experience, tree, self.seed, self.logs)\n",
        "        reward, next_state = self.env.do_action(action, self.env.turn)\n",
        "        step +=1\n",
        "        println((\"Ep_No.- \", i+1, \"Move_No.- \", step, state, self.env.turn, reward))\n",
        "        start_iter = 0\n",
        "        experience = {}\n",
        "        saved_state = {}\n",
        "        tree = None\n",
        "      step = 0\n",
        "      self.env.reset(self.agent_w,self.agent_b)\n",
        "\n",
        "  def load_backup_experiences(self) :\n",
        "    while True :\n",
        "      if not os.path.exists(os.path.join('/content/gdrive', 'My Drive', 'CHESS-AI', self.model.id, \"Exploration_Backup\")):\n",
        "        println(f'Directory created by server is missing in Google Drive this may be a synchronization issue in GDrive, retrying after 5 secs')\n",
        "        time.sleep(5)\n",
        "      else :\n",
        "        break\n",
        "    file_path = os.path.join('/content/gdrive', 'My Drive', 'CHESS-AI', self.model.id, \"Exploration_Backup\", \"exploration_\"+str(self.seed)+\".json\")\n",
        "    if not os.path.exists(file_path):\n",
        "      self.logs.to_csv(os.path.join('/content/gdrive', 'My Drive', 'CHESS-AI', self.id, \"Exploration_Backup\", \"logs_\"+str(self.seed)+\".csv\"))\n",
        "      return (0, 0, 0, {}, None, self.init_board, self.init_turn)\n",
        "    self.logs = pd.read_csv(os.path.join('/content/gdrive', 'My Drive', 'CHESS-AI', self.id, \"Exploration_Backup\", \"logs_\"+str(self.seed)+\".csv\"))\n",
        "    with open(file_path, 'r') as drive_file :\n",
        "      content = drive_file.read()\n",
        "    explr_dict = json.loads(content)\n",
        "    println(f'Backup Loaded, training resumed from Ep - {explr_dict[\"start_eps\"]}, Step - {explr_dict[\"start_step\"]} and Iteration - {explr_dict[\"start_iter\"]}')\n",
        "    return (explr_dict[\"start_eps\"], explr_dict[\"start_step\"], explr_dict[\"start_iter\"], explr_dict[\"experience\"], explr_dict[\"tree\"], explr_dict[\"init_state\"], explr_dict[\"init_turn\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRXNL6BcZwOB"
      },
      "source": [
        "# Creating TD_Search_Tree Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6vDe0JOznOp"
      },
      "outputs": [],
      "source": [
        "class TD_Tree_Search :\n",
        "  def __init__(self, model, agent, state, color, ep_no, step_no, start_iter, experience, seed) :\n",
        "    self.model = model\n",
        "    self.agent = agent\n",
        "    self.config = agent.config\n",
        "    self.state = state\n",
        "    self.color = color\n",
        "    self.ep_no = ep_no\n",
        "    self.step_no = step_no\n",
        "    self.start_iter = start_iter\n",
        "    self.seed = seed\n",
        "    self.experience = experience\n",
        "\n",
        "  def traverse(self, saved_tree) :\n",
        "    self.root = self.load_from_dict(saved_tree)\n",
        "    if not self.model.is_updated_version :\n",
        "      self.model.load_latest()\n",
        "    if self.start_iter == 0 :\n",
        "      self.save_backup_experiences(-1, self.root)\n",
        "    for self.i in range(self.start_iter, self.config[\"no_simulations\"]) :\n",
        "      startt = time.time()\n",
        "      self.rand_gen = np.random.default_rng(int(str(self.seed)+str(self.ep_no)+str(self.step_no)+str(self.i)))\n",
        "      node = self.root\n",
        "      env = Env(self.state, self.color)\n",
        "      path = []\n",
        "      while(env.game_on) :\n",
        "        if not hasattr(node, \"value\") :\n",
        "          predictions = self.model.predict(node.state, node.color)\n",
        "          node.value = predictions[0]\n",
        "          node.policy = predictions[1]\n",
        "        action = self.choose_actions(node.policy,self.config[\"epsilon\"])\n",
        "        reward, next_state = env.do_action(action,node.color)\n",
        "        path.append((node, action, reward))\n",
        "        if(action not in node.actions) :\n",
        "          break\n",
        "        node = node.children[next_state]\n",
        "      node = node.add_child(next_state, not node.color, action,reward)\n",
        "      endt = time.time()\n",
        "      temp_path = self.rollout(env, node, path)\n",
        "      endr = time.time()\n",
        "      self.backpropogate(path, temp_path)\n",
        "      node.del_children(env)\n",
        "      endb = time.time()\n",
        "      try :\n",
        "        println(f\"Iter - {self.i+1} Total {len(path)+len(temp_path)} steps in {endb-startt} secs, <Selection - Steps = {len(path)}, Time = {endt-startt}, Avg = {(endt-startt)/len(path)}>, <Rollout - Steps = {len(temp_path)}, Time = {endr-endt}, Avg = {(endr-endt)/len(temp_path)}>\")\n",
        "      except :\n",
        "        pass\n",
        "      if (self.i+1) % self.config[\"batch_size\"] == 0 :\n",
        "        self.save_backup_experiences(self.i, self.root)\n",
        "    self.rand_gen = np.random.default_rng(int(str(self.seed)+str(self.ep_no)+str(self.step_no)))\n",
        "    return self.experience\n",
        "\n",
        "  def rollout(self, env, init_node, path) :\n",
        "    node = init_node\n",
        "    temp_path = []\n",
        "    while(env.game_on) :\n",
        "      action = self.choose_actions(env.get_legal_moves(True),1)\n",
        "      reward, next_state = env.do_action(action,node.color)\n",
        "      temp_path.append((node, action, reward))\n",
        "      if len(temp_path) == 256 :\n",
        "        break\n",
        "      node = node.add_child(next_state, not node.color, action,reward)\n",
        "    self.model.bulk_predict(temp_path)\n",
        "    return temp_path\n",
        "\n",
        "  def backpropogate(self, path, temp_path) :\n",
        "    eps = path+temp_path\n",
        "    eligibility_traces_white = {state[0].state : 0.0 for state in eps if state[0].color == True}\n",
        "    eligibility_traces_black = {state[0].state : 0.0 for state in eps if state[0].color == False}\n",
        "    for i in range(len(eps)) :\n",
        "      state = eps[i]\n",
        "      if state[0].color :\n",
        "        if i < len(eps) - 2 :\n",
        "          next_state = eps[i + 2]\n",
        "          reward = eps[i][2]-eps[i+1][2]\n",
        "          delta = reward + self.config[\"discount\"] * next_state[0].value - state[0].value\n",
        "        elif i < len(eps) - 1 :\n",
        "          next_state = None\n",
        "          if eps[i+1][2] == 1 or eps[i+1][2] == -1:\n",
        "            reward = -1*eps[i+1][2]\n",
        "          else :\n",
        "            reward = eps[i][2]-eps[i+1][2]\n",
        "          delta = reward - state[0].value\n",
        "        else :\n",
        "          next_state = None\n",
        "          reward = eps[i][2]\n",
        "          delta = reward - state[0].value\n",
        "        eligibility_traces_white[state[0].state] += 1\n",
        "        for s in range(len(path)):\n",
        "          if path[s][0].color :\n",
        "            path[s][0].value += self.config[\"td_learning_rate\"] * delta * eligibility_traces_white[path[s][0].state]\n",
        "            eligibility_traces_white[path[s][0].state] *= self.config[\"discount\"] * self.config[\"lmbda\"]\n",
        "      else :\n",
        "        if i < len(eps) - 2 :\n",
        "          next_state = eps[i + 2]\n",
        "          reward = eps[i][2]-eps[i+1][2]\n",
        "          delta = reward + self.config[\"discount\"] * next_state[0].value - state[0].value\n",
        "        elif i < len(eps) - 1 :\n",
        "          next_state = None\n",
        "          reward = eps[i][2]-eps[i+1][2]\n",
        "          delta = reward - state[0].value\n",
        "        else:\n",
        "          next_state = None\n",
        "          reward = eps[i][2]\n",
        "          delta = reward - state[0].value\n",
        "        eligibility_traces_black[state[0].state] += 1\n",
        "        for s in range(len(path)):\n",
        "          if path[s][0].color == False :\n",
        "            path[s][0].value += self.config[\"td_learning_rate\"] * delta * eligibility_traces_black[path[s][0].state]\n",
        "            eligibility_traces_black[path[s][0].state] *= self.config[\"discount\"] * self.config[\"lmbda\"]\n",
        "    for i in range(len(path)) :\n",
        "      id = eps[i][0].state +\"-\"+ str(eps[i][0].color) +\"-\"+ eps[i][1]\n",
        "      self.experience[id] = eps[i][0].value\n",
        "\n",
        "  def choose_actions(self, probs, epsilon) :\n",
        "    if(self.rand_gen.random() > epsilon) :\n",
        "        return probs[0]\n",
        "    else :\n",
        "      return self.rand_gen.choice(probs)\n",
        "\n",
        "  def save_backup_experiences(self, index, root) :\n",
        "    explr_dict = {\n",
        "        \"start_eps\" : self.ep_no,\n",
        "        \"start_step\" : self.step_no,\n",
        "        \"start_iter\" : index+1,\n",
        "        \"tree\" : root.convert_to_dict(),\n",
        "        \"experience\" : self.experience,\n",
        "        \"init_state\" : self.state,\n",
        "        \"init_turn\" : self.color\n",
        "    }\n",
        "    with open(os.path.join('/content/gdrive', 'My Drive', 'CHESS-AI', self.model.id, \"Exploration_Backup\", \"exploration_\"+str(self.seed)+\".json\"), 'w') as drive_file:\n",
        "      drive_file.write(json.dumps(explr_dict, indent=2))\n",
        "    println(f\"Saved backup till Ep - {self.ep_no}, Step - {self.step_no} and Iteration - {index}\")\n",
        "\n",
        "  def load_from_dict(self, tree, parent = None) :\n",
        "    if tree is None :\n",
        "      return TD_Search_Node(self.state, self.color)\n",
        "    root = TD_Search_Node(tree[\"state\"], tree[\"color\"], parent)\n",
        "    root.actions = tree[\"actions\"]\n",
        "    for state in tree[\"children\"] :\n",
        "      root.children[state] = self.load_from_dict(tree[\"children\"][state], root)\n",
        "    return root\n",
        "\n",
        "class TD_Search_Node :\n",
        "  def __init__(self,state,color,parent_node = None) :\n",
        "    self.state = state\n",
        "    self.color = color\n",
        "    self.parent = parent_node\n",
        "    self.actions = {}\n",
        "    self.children = {}\n",
        "  def add_child(self,state,color,action,reward) :\n",
        "    node = TD_Search_Node(state,color,self)\n",
        "    self.actions[action] = [state,reward]\n",
        "    self.children[state] = node\n",
        "    return node\n",
        "\n",
        "  def convert_to_dict(self) :\n",
        "    tree = {\n",
        "        \"state\" : self.state,\n",
        "        \"color\" : self.color,\n",
        "        \"actions\" : self.actions,\n",
        "        \"children\" : {}\n",
        "    }\n",
        "    for state in self.children :\n",
        "      tree[\"children\"][state] = self.children[state].convert_to_dict()\n",
        "    return tree\n",
        "\n",
        "  def del_children(self, env) :\n",
        "    del env\n",
        "    #del self.actions\n",
        "    #del self.children\n",
        "    try :\n",
        "      del self.value\n",
        "    except :\n",
        "      pass\n",
        "    #gc.collect()\n",
        "    self.actions = {}\n",
        "    self.children = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWw2A-pjZ8qf"
      },
      "source": [
        "#Storing Training Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnJs4o48bpsO"
      },
      "outputs": [],
      "source": [
        "#storing  training configurations\n",
        "agent_w_config = {\n",
        "    \"epsilon\" : 0.78,\n",
        "    \"exploit_epsilon\" : 0,\n",
        "    \"discount\" : 0.83,\n",
        "    \"base_learning_rate\" : 0.05,\n",
        "    \"actor_learning_rate\" : 0.01,\n",
        "    \"critic_learning_rate\" : 0.05,\n",
        "    \"td_learning_rate\" : 0.1,\n",
        "    \"lmbda\" : 0.9,\n",
        "    \"batch_size\" : 32,\n",
        "    \"no_simulations\" : 32*500,   # in multiple of batch_size\n",
        "    \"actor_coefficient\" : 1,\n",
        "    \"critic_coefficient\" : 3,\n",
        "    \"entropy_coefficient\" : 0.1\n",
        "}\n",
        "agent_b_config = {\n",
        "    \"epsilon\" : 0.78,\n",
        "    \"exploit_epsilon\" : 0,\n",
        "    \"discount\" : 0.83,\n",
        "    \"base_learning_rate\" : 0.05,\n",
        "    \"actor_learning_rate\" : 0.01,\n",
        "    \"critic_learning_rate\" : 0.05,\n",
        "    \"td_learning_rate\" : 0.1,\n",
        "    \"lmbda\" : 0.9,\n",
        "    \"batch_size\" : 32,\n",
        "    \"no_simulations\" : 32*500,   # in multiple of batch_size\n",
        "    \"actor_coefficient\" : 1,\n",
        "    \"critic_coefficient\" : 3,\n",
        "    \"entropy_coefficient\" : 0.1\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBpGhawM4GJB"
      },
      "outputs": [],
      "source": [
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "def println(output) :\n",
        "  print(multiprocessing.current_process().name, output, \"\\n\")\n",
        "  sys.stdout.flush()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQnnTemaaEtm"
      },
      "source": [
        "# Training Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NwoaZOHOJZu"
      },
      "outputs": [],
      "source": [
        "# @title Training Initiate Code {display-mode: \"form\"}\n",
        "model = Model_handler(\"25_02_2024-10_52_47\")\n",
        "agent_w = Agent(agent_w_config,model,True)\n",
        "agent_b = Agent(agent_b_config,model,False)\n",
        "n_episodes = 2\n",
        "n_process = multiprocessing.cpu_count()\n",
        "#seeds = [13, 3]\n",
        "seeds = [1001, 1111]\n",
        "try :\n",
        "  if True :\n",
        "    with multiprocessing.Pool(processes = multiprocessing.cpu_count()) as pool:\n",
        "      result = pool.starmap(Trainer, zip([agent_w]*n_process, [agent_b]*n_process, [n_episodes]*n_process, seeds))\n",
        "  else :\n",
        "    trainer = Trainer(agent_w, agent_b, n_episodes, 2002)\n",
        "except Exception :\n",
        "  print(traceback.print_exc())\n",
        "  pool.terminate()\n",
        "  gc.collect()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "_HcVW0_yRLuR",
        "cDMwvNqdoQPB",
        "ueDps8HMoET5",
        "IHut_fIFoNap",
        "LFozK6bQoY2Y",
        "RRXNL6BcZwOB",
        "lWw2A-pjZ8qf",
        "7c5m_2z1C6q_"
      ],
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}